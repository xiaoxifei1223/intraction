# GRAS Time Series Forecasting Algorithm Selection and Cold Start Strategy

> Technical selection, advantages comparison, and cold start implementation solutions for GRAS indicator forecasting scenarios based on Prophet, N-BEATS, and ARIMA algorithms

---

## 1. Python Package Support

All three algorithms have mature Python ecosystem support:

### 1.1 Prophet

- **Package**: `prophet` (Open-sourced by Meta/Facebook)
- **Installation**:
  ```bash
  pip install prophet
  ```
- **Dependencies**: Requires `pystan` (heavy, long compilation time)
- **Version Recommendation**: `prophet>=1.1`
- **Documentation**: https://facebook.github.io/prophet/

### 1.2 N-BEATS

- **Package**: `darts` (Darts library contains N-BEATS implementation, recommended)
- **Installation**:
  ```bash
  pip install darts
  ```
- **Alternative**: `neuralforecast` package also has implementation
  ```bash
  pip install neuralforecast
  ```
- **Dependencies**: PyTorch (GPU support required for best performance)
- **Documentation**: https://unit8co.github.io/darts/

### 1.3 ARIMA

- **Packages**:
  - `statsmodels` (Most commonly used, comprehensive features)
  - `pmdarima` (Provides AutoARIMA, automatic hyperparameter search)
- **Installation**:
  ```bash
  pip install statsmodels
  # or
  pip install pmdarima
  ```
- **Dependencies**: Lightweight, no deep learning dependencies
- **Documentation**: https://www.statsmodels.org/

---

## 2. Algorithm Advantages Comparison

### 2.1 Prophet Advantages

#### Use Cases
- **Business metrics with obvious periodicity and holiday effects**
- Examples: Traffic metrics, error rates, user activity, etc.

#### Core Advantages
âœ… **Business-Friendly**
- Strong interpretability, clearly showing trends, seasonality, and holiday effects
- Output results are easy to explain to business stakeholders

âœ… **Missing Value Tolerance**
- Robust to missing data and outliers
- No need for perfect data preprocessing

âœ… **Holiday Modeling**
- Built-in holiday effect modeling (ideal for GRAS scenarios)
- Customizable for release windows, promotions, and other special events

âœ… **Quick to Get Started**
- Simple and intuitive API
- Few hyperparameters, low tuning cost

#### Main Limitations
âŒ Poor performance on short-term/high-frequency data  
âŒ Not suitable for complex non-linear patterns  
âŒ Long-term prediction (>30 days) accuracy degrades

#### Typical Application
```python
from prophet import Prophet

model = Prophet(
    yearly_seasonality=False,
    weekly_seasonality=True,
    daily_seasonality=False
)

# Add holidays
holidays = pd.DataFrame({
    'holiday': 'release_window',
    'ds': pd.to_datetime(['2024-01-15', '2024-02-01']),
    'lower_window': -1,
    'upper_window': 1
})
model.add_country_holidays(country_name='CN')
model = model.add_holidays(holidays)

model.fit(df_train)
forecast = model.predict(future)
```

---

### 2.2 N-BEATS Advantages

#### Use Cases
- **Large datasets, complex patterns, long-term forecasting needs**
- Examples: Capacity planning, resource forecasting, etc.

#### Core Advantages
âœ… **Deep Learning SOTA**
- Pure neural network architecture, no manual feature engineering required
- Excellent performance in M4 competition and other benchmarks

âœ… **Strong Multi-Step Forecasting**
- Can predict multiple future time points at once (e.g., next 7 days)
- Prediction accuracy degrades slowly

âœ… **Non-Linear Capability**
- Can capture complex, non-linear time series patterns
- Suitable for highly dynamic business scenarios

âœ… **Interpretable Decomposition**
- Can output trend + seasonality components (similar to Prophet)
- Supports interpretable mode

#### Main Limitations
âŒ Slow training, requires GPU acceleration  
âŒ High data volume requirements (at least hundreds of samples)  
âŒ Complex hyperparameter tuning  
âŒ Large model files, high deployment cost

#### Typical Application
```python
from darts.models import NBEATSModel
from darts import TimeSeries

# Convert to Darts time series format
series = TimeSeries.from_dataframe(df, 'timestamp', 'value')

# Train N-BEATS
model = NBEATSModel(
    input_chunk_length=30,
    output_chunk_length=7,
    n_epochs=100,
    batch_size=32
)

model.fit(series)
forecast = model.predict(n=7)
```

---

### 2.3 ARIMA Advantages

#### Use Cases
- **Short-term forecasting, small datasets, mainly linear trends**
- Examples: Short-term capacity alerts, recent trend analysis

#### Core Advantages
âœ… **Classic and Reliable**
- Solid statistical foundation, interpretable results
- Long-term validation in academia and industry

âœ… **Small Sample Friendly**
- Still effective with limited data (minimum 30-50 samples)
- Suitable for cold start scenarios

âœ… **Lightweight and Fast**
- No GPU required, extremely fast inference (milliseconds)
- Suitable for real-time online prediction

âœ… **Confidence Intervals**
- Built-in prediction interval estimation (upper and lower bounds)
- Complies with statistical standards

#### Main Limitations
âŒ Not suitable for complex seasonality (requires SARIMA extension)  
âŒ Requires stationary data (may need differencing)  
âŒ Long-term prediction degrades quickly (>7 days poor performance)  
âŒ Hyperparameter (p, d, q) selection requires experience

#### Typical Application
```python
from statsmodels.tsa.arima.model import ARIMA

# Train ARIMA
model = ARIMA(df['value'], order=(1, 1, 1))
fitted = model.fit()

# Forecast next 7 days
forecast = fitted.forecast(steps=7)
conf_int = fitted.get_forecast(steps=7).conf_int()
```

---

## 3. Algorithm Comparison Summary Table

| Dimension | Prophet | N-BEATS | ARIMA |
|-----------|---------|---------|-------|
| **Minimum Data** | 14-30 days | 100+ samples | 30-50 samples |
| **Training Speed** | Fast (seconds) | Slow (minutes-hours) | Fast (seconds) |
| **Inference Speed** | Fast | Medium | Very Fast |
| **Interpretability** | Strong | Medium | Strong |
| **Seasonality Support** | Strong (multi-level) | Strong | Weak (needs SARIMA) |
| **Holiday Modeling** | Native support | Manual features | Not supported |
| **Non-Linear Capability** | Weak | Strong | Weak |
| **Long-Term Forecast** | Medium (7-30 days) | Strong (30+ days) | Weak (3-7 days) |
| **Hyperparameter Tuning** | Simple | Complex | Medium |
| **GPU Requirement** | No | Yes | No |
| **Use Cases** | Periodic business metrics | Complex long-term forecast | Short-term rapid forecast |

---

## 4. Cold Start Strategies Without Historical Data

This is a core challenge when new GRAS applications go live. Here are four practical approaches:

### 4.1 Strategy 1: Transfer Learning â€” **Recommended**

#### Core Idea
Borrow historical data from similar applications/metrics to train a "generic model" providing initial prediction capability for new applications.

#### Implementation Steps

**Step 1: Identify Similar Applications**
- Same technology stack (e.g., all Java microservices)
- Same business type (e.g., all API gateways)
- Same metric type (e.g., all `G.error_rate`)

**Step 2: Aggregate Historical Data**
```python
# Collect historical data from multiple similar applications
similar_apps = ['app_b', 'app_c', 'app_d']
df_train = pd.concat([
    fetch_metric_history('app_b', 'G.error_rate', days=60),
    fetch_metric_history('app_c', 'G.error_rate', days=60),
    fetch_metric_history('app_d', 'G.error_rate', days=60)
])

df_train['ds'] = df_train['timestamp']
df_train['y'] = df_train['value']
```

**Step 3: Train Generic Model**
```python
from prophet import Prophet

model_generic = Prophet(
    yearly_seasonality=False,
    weekly_seasonality=True,
    daily_seasonality=False,
    changepoint_prior_scale=0.05  # Control trend flexibility
)

# Add key events for GRAS scenarios
holidays = pd.DataFrame({
    'holiday': ['release_window', 'major_promotion'],
    'ds': pd.to_datetime(['2024-01-15', '2024-02-01']),
    'lower_window': [-1, -2],
    'upper_window': [1, 1]
})
model_generic = model_generic.add_holidays(holidays)

model_generic.fit(df_train)
```

**Step 4: New Application Cold Start Usage**
```python
# New app_new goes live, immediately use generic model for prediction
future = model_generic.make_future_dataframe(periods=7)
forecast_generic = model_generic.predict(future)

# Write prediction results (marked as generic model)
write_to_db(
    app_id='app_new',
    metric_id='G.error_rate',
    forecast=forecast_generic,
    source='AI_MODEL_GENERIC',
    model_version='generic_v1.0'
)
```

**Step 5: Switch to Dedicated Model After Data Accumulation**
```python
# After 2 weeks, app_new has accumulated enough data
if len(app_new_history) >= 14:
    model_specific = Prophet()
    model_specific.fit(app_new_history)
    
    # Gradually increase dedicated model weight in decision layer
    # From generic 0.8 / specific 0.2 â†’ generic 0.2 / specific 0.8
```

#### Applicable Models
- âœ… **Prophet**: Directly merge multi-application data for training
- âœ… **N-BEATS**: Can pre-train on multi-application data, then fine-tune
- âŒ **ARIMA**: Not very suitable (too dependent on single series characteristics)

---

### 4.2 Strategy 2: Synthetic Data + Rule Generation

#### Core Idea
Use existing Java rule engine to "simulate" and generate pseudo-historical data for AI model initialization.

#### Implementation Steps

**Step 1: Backtrack History Using Rules**
```python
# Assume Java rule: error_rate = f(request_count, deploy_frequency, ...)
# Use past 30 days context data + rules to backtrack

synthetic_history = []

for day in past_30_days:
    # Get context for that day (request volume, deploy count, etc.)
    context = fetch_context(day)
    
    # Calculate theoretical metric value for that day using Java rules
    predicted_value = java_rule_engine.calculate(
        request_count=context['requests'],
        deploy_count=context['deploys'],
        is_holiday=context['is_holiday']
    )
    
    synthetic_history.append({
        'ds': day,
        'y': predicted_value
    })

df_synthetic = pd.DataFrame(synthetic_history)
```

**Step 2: Train Initial Model with Synthetic Data**
```python
model = Prophet()
model.fit(df_synthetic)

# Immediately available for prediction
forecast = model.predict(model.make_future_dataframe(periods=7))
```

**Step 3: Replace with Real Data**
```python
# Once real data is available (e.g., after 3 days)
if len(real_data) > 0:
    # Mixed training: 80% synthetic + 20% real
    df_mixed = pd.concat([
        df_synthetic.sample(frac=0.8),
        real_data
    ])
    model.fit(df_mixed)

# Gradually reduce synthetic data weight until fully using real data
```

#### Pros and Cons
- âœ… **Pros**: Better than nothing, can "get started" immediately
- âš ï¸ **Cons**: Synthetic data may not be realistic, need quick replacement with real data
- ðŸ’¡ **Recommendation**: Only as a 1-2 week transitional solution

---

### 4.3 Strategy 3: Phased Startup (Bootstrap) â€” **Stable Solution**

#### Core Idea
Phase 1 uses only Java rules, gradually introducing AI after accumulating real data.

#### Implementation Timeline

**Phase 1: Cold Start Period (0-2 weeks)**
- **Strategy**:
  - Use only Java rules for prediction
  - Write to `metric_prediction_history (source='JAVA_RULE')`
  - Simultaneously collect actual metric values â†’ `metric_value_history`
  
- **Code Example**:
  ```python
  # Decision layer configuration
  prediction_weights = {
      'JAVA_RULE': 1.0,
      'AI_MODEL': 0.0  # AI not enabled yet
  }
  
  final_prediction = (
      java_rule_prediction * prediction_weights['JAVA_RULE']
  )
  ```

**Phase 2: Mixed Period (2-4 weeks)**
- **Condition**: Data accumulated â‰¥14 days (Prophet minimum requirement)
- **Strategy**:
  - Start training Prophet/ARIMA
  - Run in parallel with rules, marked as `source='AI_MODEL'`
  - Decision layer compares both, gradually increases AI weight
  
- **Code Example**:
  ```python
  # Train first AI model
  if len(real_data) >= 14:
      model_prophet = Prophet()
      model_prophet.fit(real_data)
      
      # Gradually adjust weights
      prediction_weights = {
          'JAVA_RULE': 0.7,
          'AI_MODEL': 0.3  # Initially give 30% weight
      }
  
  # Decision layer fusion
  final_prediction = (
      java_rule_prediction * 0.7 +
      ai_prediction * 0.3
  )
  ```

**Phase 3: Mature Period (1-3 months later)**
- **Condition**: Data â‰¥100 samples
- **Strategy**:
  - Try N-BEATS (if high accuracy needed)
  - Dynamically adjust weights based on historical comparison (Actual vs Forecast MAPE)
  - May fully switch to AI (rules only as fallback)
  
- **Code Example**:
  ```python
  # Dynamic weighting based on historical accuracy
  mape_rule = calculate_mape(java_rule_history, actual_history)
  mape_ai = calculate_mape(ai_history, actual_history)
  
  # Weight inversely correlated with accuracy
  total_error = mape_rule + mape_ai
  weight_rule = (1 - mape_rule / total_error) if total_error > 0 else 0.5
  weight_ai = (1 - mape_ai / total_error) if total_error > 0 else 0.5
  
  # Normalize
  total_weight = weight_rule + weight_ai
  final_weights = {
      'JAVA_RULE': weight_rule / total_weight,
      'AI_MODEL': weight_ai / total_weight
  }
  ```

---

### 4.4 Strategy 4: External Data Enhancement

#### Core Idea
Even without historical metric values, can introduce "known event" features to assist modeling.

#### Available External Data

**1. Calendar Features**
- Weekday / Weekend
- Holidays (national statutory holidays)
- Beginning / End of month (may affect business)

**2. Business Events**
- Release windows (from CI/CD systems)
- Promotional campaigns (from operations calendar)
- System maintenance windows

**3. Global Metrics**
- Overall traffic trends (if new app migrated from old system)
- Industry benchmark data

#### Implementation Approach
```python
from prophet import Prophet
import pandas as pd

# Build "event skeleton" (no need for historical metric values)
model = Prophet()

# Add known events
holidays = pd.DataFrame({
    'holiday': 'release',
    'ds': pd.to_datetime(['2024-01-15', '2024-02-01', '2024-02-15']),
    'lower_window': -1,
    'upper_window': 1
})
model = model.add_holidays(holidays)

# Add external regressor (e.g., global traffic)
model.add_regressor('global_traffic')

# Even without target variable history, can "warm up" model structure
# When real data arrives, quickly fit
```

---

## 5. Recommended Combination Strategy for GRAS

Based on GRAS dual-source (Java rules + AI) architecture, recommended phased combination:

### 5.1 Startup Period (Data < 2 weeks)

**Primary Model**
- Java rules (deterministic, immediately available)

**Alternative Models**
- ARIMA (if any similar metric history available)
- Generic Prophet model (based on transfer learning)

**Decision Layer Configuration**
```python
prediction_weights = {
    'JAVA_RULE': 0.8,
    'AI_GENERIC': 0.2  # Generic model assistance
}
```

---

### 5.2 Growth Period (Data 2 weeks - 2 months)

**Primary Model**
- Prophet (easy to use, interpretable, suitable for periodicity)
- Dedicated model starts training

**Auxiliary Models**
- Java rules (fallback + comparison validation)
- ARIMA (for short-term prediction)

**Decision Layer Configuration**
```python
# Dynamically adjust weights
if data_quality_score > 0.8:
    prediction_weights = {
        'JAVA_RULE': 0.4,
        'PROPHET': 0.6
    }
else:
    prediction_weights = {
        'JAVA_RULE': 0.6,
        'PROPHET': 0.4
    }
```

---

### 5.3 Mature Period (Data > 2 months)

**Primary Model**
- N-BEATS (pursuing accuracy, for critical metrics)
- Or Prophet (pursuing stability, for general metrics)

**Auxiliary Models**
- ARIMA (short-term prediction, < 3 days)
- Java rules (fallback, prevent AI failure)

**Decision Layer Configuration**
```python
# Adaptive based on historical accuracy
mape_scores = {
    'JAVA_RULE': calculate_mape_last_30d('JAVA_RULE'),
    'PROPHET': calculate_mape_last_30d('PROPHET'),
    'NBEATS': calculate_mape_last_30d('NBEATS')
}

# Inverse MAPE normalized weights
weights = inverse_normalize(mape_scores)

# Final prediction
final_prediction = sum(
    model_predict(model) * weights[model]
    for model in ['JAVA_RULE', 'PROPHET', 'NBEATS']
)
```

---

## 6. Complete Cold Start Implementation Example

### 6.1 Scenario Description
- New application `app_new` just launched
- Metric: `G.error_rate`
- No historical data

### 6.2 Implementation Code

```python
import pandas as pd
from prophet import Prophet
from datetime import datetime, timedelta

# ============================================
# Step 1: Prepare "seed data" (transfer learning)
# ============================================

# Collect historical data from similar applications
similar_apps = ['app_a', 'app_b', 'app_c']
seed_data = []

for app_id in similar_apps:
    df = fetch_metric_history(
        app_id=app_id,
        metric='G.error_rate',
        days=60
    )
    df['ds'] = pd.to_datetime(df['timestamp'])
    df['y'] = df['value']
    seed_data.append(df[['ds', 'y']])

df_seed = pd.concat(seed_data, ignore_index=True)

# ============================================
# Step 2: Train generic model
# ============================================

model_generic = Prophet(
    yearly_seasonality=False,
    weekly_seasonality=True,
    daily_seasonality=False,
    changepoint_prior_scale=0.05
)

# Add GRAS key events
holidays = pd.DataFrame({
    'holiday': 'release_window',
    'ds': pd.to_datetime([
        '2024-01-15', '2024-02-01', '2024-02-15',
        '2024-03-01', '2024-03-15', '2024-04-01'
    ]),
    'lower_window': -1,
    'upper_window': 1
})

model_generic.add_country_holidays(country_name='CN')
model_generic = model_generic.add_holidays(holidays)

model_generic.fit(df_seed)

# ============================================
# Step 3: New application immediately uses generic model
# ============================================

# Predict next 7 days
future = model_generic.make_future_dataframe(periods=7)
forecast_generic = model_generic.predict(future)

# Write to database (Hot Storage)
write_prediction_to_dynamodb(
    app_id='app_new',
    metric_id='G.error_rate',
    forecast_value=forecast_generic['yhat'].iloc[-1],
    lower_bound=forecast_generic['yhat_lower'].iloc[-1],
    upper_bound=forecast_generic['yhat_upper'].iloc[-1],
    source='AI_MODEL_GENERIC',
    model_version='generic_v1.0',
    horizon='7d'
)

# Write to history table (Warm Storage)
for idx, row in forecast_generic.tail(7).iterrows():
    write_prediction_history_to_postgres(
        app_id='app_new',
        metric_id='G.error_rate',
        target_time=row['ds'],
        forecast_value=row['yhat'],
        lower_bound=row['yhat_lower'],
        upper_bound=row['yhat_upper'],
        source='AI_MODEL_GENERIC',
        model_version='generic_v1.0'
    )

# ============================================
# Step 4: Run Java rules in parallel (fallback)
# ============================================

rule_prediction = java_rule_engine.predict_error_rate(
    app_id='app_new',
    context=get_current_context()
)

write_prediction_to_dynamodb(
    app_id='app_new',
    metric_id='G.error_rate',
    forecast_value=rule_prediction,
    source='JAVA_RULE',
    model_version='rule_v1.0'
)

# ============================================
# Step 5: Decision layer fusion (cold start period weights)
# ============================================

weights = {
    'JAVA_RULE': 0.7,        # Rules more reliable
    'AI_MODEL_GENERIC': 0.3  # Generic model assistance
}

final_prediction = (
    rule_prediction * weights['JAVA_RULE'] +
    forecast_generic['yhat'].iloc[-1] * weights['AI_MODEL_GENERIC']
)

# Write final prediction (marked as FUSION)
write_prediction_to_dynamodb(
    app_id='app_new',
    metric_id='G.error_rate',
    forecast_value=final_prediction,
    source='FUSION',
    model_version='bootstrap_v1.0'
)

# ============================================
# Step 6: Periodic check, switch to dedicated model
# ============================================

def check_and_upgrade_model(app_id, metric_id):
    """
    Scheduled task: Check data accumulation, decide whether to switch model
    """
    real_data = fetch_metric_history(app_id, metric_id, days=30)
    
    if len(real_data) >= 14:
        print(f"[{app_id}] Sufficient data, training dedicated model...")
        
        # Train dedicated Prophet
        model_specific = Prophet()
        model_specific.fit(real_data)
        
        # Update weight configuration
        update_decision_weights(
            app_id=app_id,
            weights={
                'JAVA_RULE': 0.5,
                'AI_MODEL_SPECIFIC': 0.5  # Dedicated model weight increased
            }
        )
        
        print(f"[{app_id}] Switched to dedicated model")
    
    if len(real_data) >= 100:
        print(f"[{app_id}] Data mature, trying N-BEATS...")
        # Train N-BEATS (if needed)
        # ...

# Run check once daily
schedule.every().day.at("02:00").do(
    check_and_upgrade_model,
    app_id='app_new',
    metric_id='G.error_rate'
)
```

---

## 7. Key Decision Points Summary

### 7.1 Model Selection Decision Tree

```
START
  |
  â”œâ”€ Data < 2 weeks?
  â”‚    â”œâ”€ YES â†’ Use Java rules + Generic Prophet (transfer learning)
  â”‚    â””â”€ NO â†’ Continue
  |
  â”œâ”€ Data 2 weeks - 2 months?
  â”‚    â”œâ”€ Obvious periodicity? â†’ Prophet (recommended)
  â”‚    â”œâ”€ Short-term forecast < 7 days? â†’ ARIMA
  â”‚    â””â”€ Poor data quality? â†’ ARIMA (robust)
  |
  â””â”€ Data > 2 months?
       â”œâ”€ Pursue extreme accuracy? â†’ N-BEATS
       â”œâ”€ Pursue stable reliability? â†’ Prophet
       â””â”€ Multi-model fusion â†’ Prophet + ARIMA + N-BEATS (ensemble)
```

### 7.2 Cold Start Strategy Priority

1. **Transfer Learning** (Most Recommended)
   - Applicable: Have similar application history
   - Effectiveness: â˜…â˜…â˜…â˜…â˜…
   - Implementation Difficulty: â˜…â˜…â˜…â˜†â˜†

2. **Phased Startup** (Most Stable)
   - Applicable: Any scenario
   - Effectiveness: â˜…â˜…â˜…â˜…â˜†
   - Implementation Difficulty: â˜…â˜…â˜†â˜†â˜†

3. **Synthetic Data** (Fast but less accurate)
   - Applicable: Rule engine well-developed
   - Effectiveness: â˜…â˜…â˜…â˜†â˜†
   - Implementation Difficulty: â˜…â˜…â˜…â˜…â˜†

4. **External Data Enhancement** (Auxiliary method)
   - Applicable: Combined with other strategies
   - Effectiveness: â˜…â˜…â˜†â˜†â˜†
   - Implementation Difficulty: â˜…â˜…â˜…â˜†â˜†

---

## 8. Monitoring and Evaluation Metrics

### 8.1 Model Performance Metrics

Need to continuously monitor the following metrics in Decision layer:

```python
# 1. MAPE (Mean Absolute Percentage Error)
mape = np.mean(np.abs((actual - predicted) / actual)) * 100

# 2. RMSE (Root Mean Square Error)
rmse = np.sqrt(np.mean((actual - predicted) ** 2))

# 3. Coverage (Prediction interval coverage rate)
coverage = np.mean(
    (actual >= lower_bound) & (actual <= upper_bound)
)

# 4. Lead time accuracy
# For GRAS, key is whether can provide X hours advance warning
early_warning_accuracy = calculate_early_warning_hit_rate(
    predictions=predictions,
    actuals=actuals,
    lead_time_hours=2
)
```

### 8.2 Write to Monitoring Table

```sql
-- Record model evaluation in PostgreSQL
CREATE TABLE model_evaluation (
    id              BIGSERIAL PRIMARY KEY,
    app_id          VARCHAR(100) NOT NULL,
    metric_id       VARCHAR(100) NOT NULL,
    model_source    VARCHAR(50) NOT NULL,  -- 'PROPHET'/'NBEATS'/'ARIMA'/'JAVA_RULE'
    eval_date       DATE NOT NULL,
    mape            DOUBLE PRECISION,
    rmse            DOUBLE PRECISION,
    coverage        DOUBLE PRECISION,
    sample_count    INT,
    created_at      TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE INDEX idx_model_eval_app_metric_date
    ON model_evaluation (app_id, metric_id, eval_date);
```

---

## 9. References

### Official Documentation
- Prophet: https://facebook.github.io/prophet/
- Darts (N-BEATS): https://unit8co.github.io/darts/
- Statsmodels (ARIMA): https://www.statsmodels.org/

### Papers
- N-BEATS: Neural basis expansion analysis for interpretable time series forecasting (ICLR 2020)
- Prophet: Forecasting at Scale (PeerJ 2018)

### Best Practices
- AWS Forecast Best Practices: https://docs.aws.amazon.com/forecast/
- Google Cloud AI Platform Time Series: https://cloud.google.com/solutions/machine-learning/
